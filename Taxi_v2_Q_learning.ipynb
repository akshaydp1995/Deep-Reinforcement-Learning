{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taxi-v2-Q-learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaydp1995/Deep-Reinforcement-Learning/blob/master/Taxi_v2_Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "HGDk1oiJfWmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import sys\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mAsDIE9nfh86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e95be8e5-3d45-4a97-d802-9b505ada6290"
      },
      "cell_type": "code",
      "source": [
        "# Create an environment using OpenAI's Gym Toolkit\n",
        "\n",
        "env = gym.make('Taxi-v2')   # Create an environment\n",
        "gamma = 0.9\n",
        "\n",
        "print(\"Observation Space  :\", env.observation_space)     # State Space\n",
        "print(\"Action Space       :\", env.action_space)          # Action Space\n",
        "print(\"Reward Range       :\", env.reward_range)          # Reward Range\n",
        "print(\"Discount Factor    :\", gamma)                     # Discount Factor"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Space  : Discrete(500)\n",
            "Action Space       : Discrete(6)\n",
            "Reward Range       : (-inf, inf)\n",
            "Discount Factor    : 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D7HUffMwiDuT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f2b6d89-bd14-4518-af52-fb05ba275c9c"
      },
      "cell_type": "code",
      "source": [
        "def probs(Q, nA, state, epsilon):\n",
        "    policy_s = np.ones(nA) * epsilon / nA\n",
        "    best_a = np.argmax(Q[state])\n",
        "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
        "    return policy_s  \n",
        "  \n",
        "def q_learning(env, nA, num_episodes, alpha, gamma=1.0):\n",
        "    rewards = 0\n",
        "    Q = defaultdict(lambda: np.zeros(nA))\n",
        "    epsilon1 = 0.6\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "        if i_episode in range(num_episodes-99, num_episodes+1):\n",
        "          rewards += reward\n",
        "        state = env.reset()\n",
        "        epsilon = max(epsilon1 - 0.0001, 0.1)\n",
        "        if i_episode % 100 == 0:\n",
        "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()   \n",
        "        while True: \n",
        "            action = np.random.choice(range(6), p = probs(Q, nA, state, epsilon))\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_action = np.argmax(Q[next_state])\n",
        "            Q[state][action] = Q[state][action] + alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action])\n",
        "            state = next_state\n",
        "            if done: \n",
        "              break\n",
        "    return Q, rewards\n",
        "  \n",
        "episodes = 10000\n",
        "Q_final, reward_total = q_learning(env, 6, episodes, 0.01, gamma)\n",
        "print(\"\\nAverage Reward over the last 100 consecutive trials\", reward_total/100)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 10000/10000Average Reward 20.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}