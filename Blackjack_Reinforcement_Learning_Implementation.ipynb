{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Blackjack Reinforcement Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaydp1995/Deep-Reinforcement-Learning/blob/master/Blackjack_Reinforcement_Learning_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eR2sFIs1FP3h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Game rules: \n",
        "\n",
        "Blackjack is a card game where the player plays against the dealer. \n",
        "The game starts off with 2 cards each, where only one is open for the other person to view and one is closed. The player can hit (1) and continue to be dealt cards. He/she can stick(0) when he/she thinks that the sum of all his cards is close but less than 21. If the sum is greater than 21, he goes bust and the dealer wins. Once the player sticks, the dealer is now dealt cards until the sum of the dealer's cards is greater than or equal to 17. If the sum of the dealer's cards is greater than 21, the dealer goes bust and the player wins. At the end, if both do not go bust, the sum of the cards is compared and the one closest to 21 wins."
      ]
    },
    {
      "metadata": {
        "id": "_m9W0mM1EMxd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Monte Carlo Method on OpenAI gym's Blackjack environment - My implementation**"
      ]
    },
    {
      "metadata": {
        "id": "QjyCTUzaEZ_J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing the necessary libraries"
      ]
    },
    {
      "metadata": {
        "id": "1KkMss3vD_Fh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "drEAuEjhEc2B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating the environment"
      ]
    },
    {
      "metadata": {
        "id": "xrxO6hsiEDam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "036fa63b-8015-4ec2-e8d2-0d2f2df3ac82"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v0')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CLnxYhQmE9ag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Observation Space - 3 tuples (0-31 *player card sum*, 0-10 or *dealer showing card*, 0-1 *binary player usable ace*) - 704 possible states\n",
        "\n",
        "Action Space - Either Stick (0) or Hit (1)"
      ]
    },
    {
      "metadata": {
        "id": "RSkOuxOrEFzk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1f0894cc-29ca-48cc-ef5a-ec789b4ee115"
      },
      "cell_type": "code",
      "source": [
        "print(\"Observation Space - \", env.observation_space)\n",
        "print(\"Action Space -      \", env.action_space)\n",
        "print(\"Reward -             Discrete(3)\") # Reward is -1 if player loses, 0 if draw, 1 if player wins"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Space -  Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
            "Action Space -       Discrete(2)\n",
            "Reward -             Discrete(3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wXvWJwNeEH64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "ef51a25e-776f-48d4-98e1-39871afdea3a"
      },
      "cell_type": "code",
      "source": [
        "for episode in range(5):                           # 5 episodes or episodic tasks\n",
        "  state = env.reset()                              # reset environment\n",
        "  print(\"** New Game! **\")\n",
        "  while True:     \n",
        "    print(state) \n",
        "    action = env.action_space.sample()             # choosing a random action (equiprobable random policy followed here)\n",
        "    state, reward, done, info = env.step(action)   # running an episode \n",
        "    if done:    \n",
        "      print('Reward: ', reward)\n",
        "      print('You won :)') if reward > 0 else print('You lost :(')\n",
        "      print(\"Game Ends!\\n\")\n",
        "      break                                        # if the Blackjack game ends, break out of the loop and start the next episode"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** New Game! **\n",
            "(19, 10, False)\n",
            "Reward:  1.0\n",
            "You won :)\n",
            "Game Ends!\n",
            "\n",
            "** New Game! **\n",
            "(13, 6, False)\n",
            "(20, 6, False)\n",
            "Reward:  1.0\n",
            "You won :)\n",
            "Game Ends!\n",
            "\n",
            "** New Game! **\n",
            "(9, 1, False)\n",
            "Reward:  -1.0\n",
            "You lost :(\n",
            "Game Ends!\n",
            "\n",
            "** New Game! **\n",
            "(12, 5, False)\n",
            "Reward:  1.0\n",
            "You won :)\n",
            "Game Ends!\n",
            "\n",
            "** New Game! **\n",
            "(19, 2, False)\n",
            "Reward:  1.0\n",
            "You won :)\n",
            "Game Ends!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EWUVvYq4benD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Monte Carlo Prediction**"
      ]
    },
    {
      "metadata": {
        "id": "-F51cf7PZRlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_episode(env):\n",
        "  episode = []                                           # One episode\n",
        "  state = env.reset()                                    \n",
        "  while True:\n",
        "    probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8]  # Setting the policy - 80% probability of sticking if player card sum > 18, 20% hit if not \n",
        "    action = np.random.choice(np.arange(2), p=probs)      \n",
        "    next_state, reward, done, info = env.step(action)    # Acting in the environment\n",
        "    episode.append(next_state)\n",
        "    episode.append(action)\n",
        "    episode.append(reward)\n",
        "    state = next_state                                   # If game doesn't end here, go to the next state\n",
        "    if done:\n",
        "      break\n",
        "  return episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgCxZ26R1uD-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Testing if generation of episode works"
      ]
    },
    {
      "metadata": {
        "id": "dHyR-CX9slBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1525b816-d2d0-4659-c710-3f04a6ad4e2c"
      },
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(generate_episode(env))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(20, 5, False), 0, 1.0]\n",
            "[(11, 3, False), 0, -1.0]\n",
            "[(15, 4, False), 0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EE4MfZvp1105",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Q-table updation"
      ]
    },
    {
      "metadata": {
        "id": "-NWeXMkd1UiO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
        "    # initialize empty dictionaries of arrays\n",
        "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    # loop over episodes\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "        states = []\n",
        "        actions = []\n",
        "        # monitor progress\n",
        "        if i_episode % 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "        ## TODO: complete the function\n",
        "        epi = generate_episode(env)\n",
        "        for each in epi:\n",
        "            state = epi[0][0]\n",
        "            action = epi[0][1]\n",
        "            if state not in states and action not in actions:\n",
        "                g = epi[-1]\n",
        "                N[state,action] = N[state,action] + 1\n",
        "                returns_sum[state,action] = returns_sum[state,action] + g\n",
        "                break\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            Q[state][action] = (returns_sum[state][action])/(N[state][action])\n",
        "    return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbpYhjhL6VJa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training"
      ]
    },
    {
      "metadata": {
        "id": "l6M0PrRe15Wi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2711a33-852c-4b89-f15c-ed7876460156"
      },
      "cell_type": "code",
      "source": [
        "Q = mc_prediction_q(env, 5000, generate_episode)  # obtain the action-value function"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 5000/5000."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UyzoWsbz2VxS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}